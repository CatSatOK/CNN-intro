{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Cat or dog CNN with Keras**\n",
    "\n",
    "First task is to organise the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# create directories\n",
    "dataset_home = 'dataset/'\n",
    "subdirs = ['single_prediction/', 'training_set/', 'test_set/']\n",
    "for subdir in subdirs:\n",
    "    # create label subdirectories\n",
    "    labeldirs = ['dogs/', 'cats/']\n",
    "    for labldir in labeldirs:\n",
    "        newdir = dataset_home + subdir + labldir\n",
    "        os.makedirs(newdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Creating the CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librairies\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import BatchNormalization\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to create the model\n",
    "def define_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same', input_shape=(64, 64, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#iniatalisation of model\n",
    "model = define_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Training the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "%time\n",
    "# create data generator\n",
    "datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "# prepare iterators\n",
    "train_it = datagen.flow_from_directory('dataset/training_set/',class_mode='binary', batch_size=32, target_size=(64, 64))\n",
    "test_it = datagen.flow_from_directory('dataset/test_set/',class_mode='binary', batch_size=32, target_size=(64, 64))\n",
    "model.summary()\n",
    "\n",
    "# fit model\n",
    "history = model.fit(train_it, steps_per_epoch=250, validation_data=test_it, validation_steps=63, epochs=20)\n",
    "#I've specified steps instead of doing them off length to try and speed up (8000/32=250 and 2000/32=62.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "_, acc = model.evaluate_generator(test_it, steps=len(test_it), verbose=0)\n",
    "print('> %.3f' % (acc * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(history):\n",
    "    # plot loss\n",
    "    plt.subplot(211)\n",
    "    plt.title('Cross Entropy Loss')\n",
    "    plt.plot(history.history['loss'], color='blue', label='train')\n",
    "    plt.plot(history.history['val_loss'], color='orange', label='test')\n",
    "    # plot accuracy\n",
    "    plt.subplot(212)\n",
    "    plt.title('Classification Accuracy')\n",
    "    plt.plot(history.history['accuracy'], color='blue', label='train')\n",
    "    plt.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "    # save plot to file\n",
    "    #filename = sys.argv[0].split('/')[-1]\n",
    "    #plt.savefig(filename + '_plot.png')\n",
    "    #plt.close()\n",
    "    \n",
    "summarize_diagnostics(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Predicting on new images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to make prediction path method\n",
    "from keras.preprocessing import image\n",
    "\n",
    "def new_prediction(path):\n",
    "    test_image = image.load_img(path, target_size = (64, 64))\n",
    "    test_image = image.img_to_array(test_image)\n",
    "    test_image = np.expand_dims(test_image, axis=0)\n",
    "    results = model.predict(test_image)\n",
    "    \n",
    "    if results[0][0] == 1:\n",
    "        return 'dog'\n",
    "    else:\n",
    "        return 'cat'\n",
    "\n",
    "#new_prediction('dataset/single_prediction/cat_or_dog_1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename = 'dataset/single_prediction/cat_or_dog_1.jpg', width=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Improving the model **\n",
    "\n",
    "#hyperparamter improvement options :\n",
    "L2 regularization, Learning Rate (LR) optimization, Batch Size increase, Increase model capacity \n",
    "\n",
    "#data redesign techniques for improvement : \n",
    "Increase image resolution (progressive resizing), Random image rotations >> Change orientation of the image, Random image shifts, Vertical and horizontal shift\n",
    "\n",
    "#Techniques for performance improvement with model optimization:\n",
    "-Fine tuning the model with subset data >> Dropping few data samples for some of the overly sampled data classes\n",
    "-Class weights >> Used to train highly imbalanced (biased) database, class weights will give equal importance to all the classes during training\n",
    "-Fine tuning the model with train data >> Use the model to predict on training data, retrain the model for the wrongly predicted images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#improved model\n",
    "def define_model_2():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(64, 64, 3)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.001), padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = SGD(lr=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if wanted to generate data at other anle to test it\n",
    "# create data generators\n",
    "#train_datagen = ImageDataGenerator(rescale=1.0/255.0,width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "#test_datagen = ImageDataGenerator(rescale=1.0/255.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another solution - transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import vgg16 \n",
    "\n",
    "# define cnn model\n",
    "def define_model_transfer_learning():\n",
    "    # load model\n",
    "    model = VGG16(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "    # mark loaded layers as not trainable\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    # add new classifier layers\n",
    "    flat1 = Flatten()(model.layers[-1].output)\n",
    "    class1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "    output = Dense(1, activation='sigmoid')(class1)\n",
    "    # define new model\n",
    "    model = Model(inputs=model.inputs, outputs=output)\n",
    "    # compile model\n",
    "    opt = SGD(lr=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the vgg16 model\n",
    "classifier_vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
    "\n",
    "# set hidden layers as untrainable (not compulsory)\n",
    "for layer in classifier_vgg16.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# add new layers for our classification problem\n",
    "flat = Flatten()(classifier_vgg16.output)\n",
    "classif = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat)\n",
    "output = Dense(1, activation='sigmoid')(classif)\n",
    "\n",
    "classifier_vgg16 = Model(inputs=classifier_vgg16.inputs, outputs=output)\n",
    "\n",
    "classifier_vgg16.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#training\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                 target_size = (150, 150),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size = (150, 150),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')\n",
    "\n",
    "classifier_vgg16.fit(training_set,\n",
    "                     steps_per_epoch = 250, #8000/32\n",
    "                     epochs = 8,\n",
    "                     validation_data = test_set,\n",
    "                     validation_steps = 63) #2000/32=62.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
